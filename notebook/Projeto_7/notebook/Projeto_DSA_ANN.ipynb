{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Formação Cientista de Dados - Data Science Academy\n",
        "\n",
        "# **Projeto com feedback 7 - Medicina Personalizada - Redefinindo o Tratamento de Câncer**\n",
        "\n",
        "**Nome**: Rafael Henrique Gallo\n",
        "\n",
        "**Descrição**: Muito tem sido dito durante os últimos anos sobre como a medicina de\n",
        "precisão e, mais concretamente, como o teste genético, vai provocar disrupção no\n",
        "tratamento de doenças como o câncer.\n",
        "Mas isso ainda está acontecendo apenas parcialmente devido à enorme\n",
        "quantidade de trabalho manual ainda necessário. Neste projeto, tentaremos levar\n",
        "a medicina personalizada ao seu potencial máximo.\n",
        "Uma vez sequenciado, um tumor cancerígeno pode ter milhares de\n",
        "mutações genéticas. O desafio é distinguir as mutações que contribuem para o\n",
        "crescimento do tumor das mutações.\n",
        "Atualmente, esta interpretação de mutações genéticas está sendo feita\n",
        "manualmente. Esta é uma tarefa muito demorada, onde um patologista clínico tem\n",
        "que revisar manualmente e classificar cada mutação genética com base em\n",
        "evidências da literatura clínica baseada em texto.\n",
        "Para este projeto, o MSKCC (Memorial Sloan Kettering Cancer Center) está\n",
        "disponibilizando uma base de conhecimento anotada por especialistas, onde\n",
        "pesquisadores e oncologistas de nível mundial anotaram manualmente milhares\n",
        "de mutações.\n",
        "\n",
        "**Objetivo problema**: Neste projeto, você vai desenvolver um algoritmo de Aprendizado de Máquina que, usando essa base de conhecimento como uma linha de base, classifica automaticamente as variações genéticas.\n",
        "\n",
        "- Recomendamos o uso de Python para criação de um modelo de Deep Learning com Keras.\n",
        "\n",
        "**O dataset completo pode ser encontrado em**:\n",
        "https://www.kaggle.com/c/msk-redefining-cancer-treatment/data"
      ],
      "metadata": {
        "id": "iLAF-Anfs8cp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Descrição projeto**\n",
        "\n",
        "Nesse projeto, desenvolvi uma rede neural com o objetivo de classificar as variações genéticas. Utilizei modelos de aprendizado de máquina para processamento de linguagem natural, como o Word2Vec e o Doc2Vec, para extrair informações relevantes das palavras-chave. Em seguida, utilizei o modelo de extração de tópicos LDA para identificar e extrair as palavras-chave pertinentes. Por fim, implementei uma rede neural para classificar as variações genéticas.\n",
        "\n",
        "O modelo Word2Vec é uma técnica popular que mapeia palavras em vetores de alta dimensionalidade, capturando relações semânticas e sintáticas entre elas. Essa abordagem permitiu que a rede neural obtivesse uma compreensão mais profunda do significado das palavras relacionadas às variações genéticas.\n",
        "\n",
        "O Doc2Vec, por sua vez, expande o conceito do Word2Vec para documentos inteiros, atribuindo vetores a cada documento. Essa técnica possibilitou a extração de informações contextuais relevantes das variações genéticas, considerando o texto completo em que elas estão inseridas.\n",
        "\n",
        "Além disso, apliquei o modelo de extração de tópicos LDA (Latent Dirichlet Allocation) para identificar os tópicos principais presentes nos documentos relacionados às variações genéticas. Essa abordagem permitiu a obtenção de palavras-chave específicas de cada tópico, fornecendo uma visão mais abrangente e organizada das informações relevantes.\n",
        "\n",
        "Por fim, desenvolvi uma rede neural para classificar as variações genéticas com base nas informações extraídas pelos modelos anteriores. Essa rede neural foi treinada utilizando conjuntos de dados rotulados, nos quais cada variação genética estava associada a uma classe específica. Com a rede neural treinada, foi possível realizar classificações precisas de novas variações genéticas com base nos padrões aprendidos durante o treinamento.\n",
        "\n",
        "Em suma, por meio da combinação dos modelos de processamento de linguagem natural Word2Vec, Doc2Vec e extração de tópicos LDA, juntamente com uma rede neural para classificação, obtive um sistema eficiente para a classificação de variações genéticas. Essa abordagem possibilitou a análise e compreensão mais precisa das informações genéticas, contribuindo para avanços na área da genômica e sua aplicação em diversas áreas da saúde e biologia."
      ],
      "metadata": {
        "id": "rkhMtpTkt641"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixando pacotes necesarios\n",
        "!pip install watermark\n",
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "id": "nU6FxufgTKQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxwqKEk2LsA5"
      },
      "outputs": [],
      "source": [
        "#Importação das bibliotecas\n",
        "\n",
        "# Bibliotecas sistema\n",
        "import os\n",
        "import re\n",
        "import tqdm\n",
        "import string\n",
        "import unicodedata\n",
        "import itertools\n",
        "\n",
        "# Biblioteca para manipulação de arquivos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.linalg import lstsq\n",
        "from numpy.random import randn, seed\n",
        "\n",
        "# Bibliotecas de visualização\n",
        "import seaborn as sns\n",
        "import matplotlib as m\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Bibliotecas rede neural\n",
        "import tensorflow\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Bibliotecas para extração tópicos\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# Vetorização palavras\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Importando NLTK NLP\n",
        "# Baixando pacote do nltk\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import conll2000\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from nltk.chunk import ne_chunk\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Baixando todos os complementos NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Versão do python\n",
        "from platform import python_version\n",
        "print('Versão Jupyter Notebook neste projeto:', python_version())\n",
        "\n",
        "# Carregar as versões das bibliotecas\n",
        "import watermark\n",
        "\n",
        "# Warnings retirar alertas\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Configuração fundo dos gráficos e estilo, tamanho da fonte\n",
        "sns.set_style('whitegrid')\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "seed(42)\n",
        "\n",
        "# Verficações da versões das bibliotecas\n",
        "%reload_ext watermark\n",
        "%watermark -a \"Rafael Gallo\" --iversions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Base dados**"
      ],
      "metadata": {
        "id": "wVgp-wjJvBid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tabelas\n",
        "data_train = pd.read_csv('training_variants')\n",
        "test_data = pd.read_csv('test_variants')\n",
        "data_train_text = pd.read_csv('training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
        "text_data = pd.read_csv('test_text', sep='\\|\\|', engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
        "data1 = pd.read_csv('stage1_solution_filtered.csv')\n",
        "\n",
        "# Merge tabelas\n",
        "data = pd.merge(data_train, data_train_text, how='left', on='ID').fillna('')\n",
        "train = pd.merge(data_train, data_train_text, how='left', on='ID').fillna('')\n",
        "train = pd.merge(data_train, data_train_text, how='left', on='ID').fillna('')\n",
        "test = pd.merge(test_data, text_data, how='left', on='ID').fillna('')\n",
        "\n",
        "# Trasformando tabela para númerica\n",
        "data1['Class'] = pd.to_numeric(data1.drop('ID', axis=1).idxmax(axis=1).str[5:]).fillna(0).astype(np.int64)\n",
        "\n",
        "# Index na tabela\n",
        "test = test[test.index.isin(data1['ID'])]\n",
        "\n",
        "# Merge nova tabela\n",
        "df = test.merge(data1[['ID', 'Class']], on='ID', how='left')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "cYqAXnQtL5Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando os 5 últimos dados\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "S-ulV2E9RL6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Info dos dados\n",
        "df.info()"
      ],
      "metadata": {
        "id": "5FevLEtrRL-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tipos dos dados\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "Qo_-eSqzRMB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "ax = df['Gene'].value_counts().plot(kind='area')\n",
        "\n",
        "ax.get_xaxis().set_ticks([])\n",
        "ax.set_title('Gene Frequency Plot')\n",
        "ax.set_xlabel('Gene')\n",
        "ax.set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "enIymovDRMFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "ax = df['Class'].value_counts().plot(kind='bar')\n",
        "\n",
        "ax.set_title('Class Distribution Over Entries')\n",
        "ax.set_xlabel('Class')\n",
        "ax.set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fyJShvolRMIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Gráfico de nuvem palavras mais faladas com dataset limpo\n",
        "todos_palavras2 = ' '.join([message for message in df[\"Text\"]])\n",
        "nuvem_palavras2 = WordCloud(width = 900,\n",
        "                           height = 600,\n",
        "                           background_color=\"white\",\n",
        "                           collocations = True,\n",
        "                           max_font_size = 200).generate(todos_palavras2)\n",
        "\n",
        "plt.figure(figsize= (18.5, 25))\n",
        "plt.title(\"Nuvem palavras limpo\")\n",
        "plt.ylabel(\"Total\")\n",
        "plt.xlabel(\"Nuvem\")\n",
        "plt.imshow(nuvem_palavras2, interpolation = 'bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nNUdS5k_RMPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Gráfico de nuvem palavras mais faladas com dataset limpo\n",
        "todos_palavras2 = ' '.join([message for message in df[\"Text\"]])\n",
        "nuvem_palavras2 = WordCloud(width = 900,\n",
        "                           height = 600,\n",
        "                           background_color=\"black\",\n",
        "                           collocations = True,\n",
        "                           max_font_size = 200).generate(todos_palavras2)\n",
        "\n",
        "plt.figure(figsize= (18.5, 25))\n",
        "plt.title(\"Nuvem palavras limpo\")\n",
        "plt.ylabel(\"Total\")\n",
        "plt.xlabel(\"Nuvem\")\n",
        "plt.imshow(nuvem_palavras2, interpolation = 'bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6aukdtfKTYpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# Gráfico de nuvem palavras mais faladas com dataset limpo\n",
        "todos_palavras2 = ' '.join([message for message in df[\"Variation\"]])\n",
        "nuvem_palavras2 = WordCloud(width = 900,\n",
        "                           height = 600,\n",
        "                           background_color=\"white\",\n",
        "                           collocations = True,\n",
        "                           max_font_size = 200).generate(todos_palavras2)\n",
        "\n",
        "plt.figure(figsize= (18.5, 25))\n",
        "plt.title(\"Nuvem palavras - Variation\")\n",
        "plt.ylabel(\"Total\")\n",
        "plt.xlabel(\"Nuvem\")\n",
        "plt.imshow(nuvem_palavras2, interpolation = 'bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ASmRVYqYTj6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pré-processamento**"
      ],
      "metadata": {
        "id": "d2PUXuPAY1hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pré-processamento dos dados, se necessário\n",
        "# Exemplo: converter para minúsculas e dividir o texto em palavras\n",
        "df['Text'] = df['Text'].str.lower()\n",
        "df['Text'] = df['Text'].str.split()"
      ],
      "metadata": {
        "id": "Uhiot-HCMDTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crie uma lista de listas de tokens\n",
        "sentences = df['Text'].tolist()"
      ],
      "metadata": {
        "id": "RoE_QiQxMiWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Modelo Word2Vec**\n",
        "\n",
        "- Word2Vec é um modelo de processamento de linguagem natural (NLP) amplamente utilizado para representar palavras em forma de vetores. Ele foi desenvolvido por Mikolov et al. em 2013 como uma técnica eficaz para aprender representações de palavras em um espaço vetorial contínuo.\n",
        "\n",
        "\n",
        "- A principal ideia por trás do Word2Vec é capturar o significado e as relações entre as palavras com base no contexto em que elas ocorrem. O modelo é treinado em grandes quantidades de texto não rotulado, como coleções de documentos ou até mesmo a internet, para aprender representações distribuídas de palavras. O Word2Vec utiliza uma abordagem de aprendizado não supervisionado, na qual palavras que aparecem em contextos similares tendem a ter representações vetoriais semelhantes. O modelo faz isso ao prever a probabilidade de ocorrência de uma palavra em relação às palavras próximas a ela em um determinado contexto. Essa previsão é realizada por meio de duas arquiteturas principais: Skip-gram e CBOW (Continuous Bag-of-Words).\n",
        "\n",
        "\n",
        "- No modelo Skip-gram, a palavra central é usada para prever as palavras do contexto. Por exemplo, dada a frase \"o gato está no telhado\", o modelo Skip-gram tentaria prever a palavra \"gato\" com base nas palavras \"o\", \"está\", \"no\" e \"telhado\". Já no modelo CBOW, as palavras do contexto são usadas para prever a palavra central. Uma vez treinado, o modelo Word2Vec gera vetores de palavras em um espaço de alta dimensionalidade, em que palavras semanticamente relacionadas estão próximas umas das outras. Esses vetores podem ser usados como recursos para uma variedade de tarefas de processamento de linguagem natural, como classificação de texto, agrupamento de documentos, tradução automática, entre outros.\n",
        "\n"
      ],
      "metadata": {
        "id": "NGl22MsyY6A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treine o modelo Word2Vec\n",
        "\n",
        "# Importando biblioteca\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Criando modelo\n",
        "model = Word2Vec(sentences,\n",
        "                 window=5,\n",
        "                 min_count=5,\n",
        "                 workers=4)"
      ],
      "metadata": {
        "id": "hiVOMCtGMniT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salve o modelo treinado\n",
        "model.save(\"modelo_word2vec.bin\")"
      ],
      "metadata": {
        "id": "Xe23E0yYMqvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregue o modelo treinado\n",
        "model = Word2Vec.load(\"modelo_word2vec.bin\")\n",
        "\n",
        "# Obtenha o vetor de uma palavra específica\n",
        "word_vector = model.wv['cancer']\n",
        "print(\"Vetor da palavra 'cancer':\", word_vector)"
      ],
      "metadata": {
        "id": "h4VbOkOpM15n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontre as palavras mais similares a uma palavra específica\n",
        "similar_words = model.wv.most_similar('treatment')\n",
        "print(\"Palavras similares a 'treatment':\", similar_words)"
      ],
      "metadata": {
        "id": "y1Azna23M4fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realize operações com palavras\n",
        "result = model.wv.most_similar(positive=['gene', 'mutation'],\n",
        "                               negative=['cancer'])\n",
        "print(\"Palavra mais similar a 'gene' e 'mutation', menos similar a 'cancer':\", result)"
      ],
      "metadata": {
        "id": "emrYNlUcNJZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Doc2Vec**\n",
        "\n",
        "- Doc2Vec, também conhecido como Paragraph Vector, é um modelo de processamento de linguagem natural (NLP) que estende o conceito do Word2Vec para a representação de documentos inteiros em forma de vetores. Foi proposto por Le and Mikolov em 2014 como uma abordagem para capturar o contexto e o significado dos documentos de maneira distribuída. Assim como o Word2Vec, o Doc2Vec é baseado em aprendizado não supervisionado e utiliza redes neurais para aprender representações vetoriais de documentos. A ideia principal é atribuir um vetor único a cada documento, permitindo que ele seja comparado e relacionado a outros documentos de forma eficiente.\n",
        "\n",
        "- O modelo Doc2Vec pode ser implementado de duas maneiras principais: PV-DM (Distributed Memory Paragraph Vector) e PV-DBOW (Distributed Bag-of-Words Paragraph Vector). No PV-DM, o modelo tenta prever a próxima palavra em um contexto, considerando tanto as palavras do documento quanto o próprio vetor do documento. Ele captura tanto o contexto local (palavras próximas) quanto o contexto global (o documento em si) durante o treinamento.\n",
        "\n",
        "- No PV-DBOW, o modelo ignora as palavras do documento e concentra-se apenas em prever as palavras com base no vetor do documento. Dessa forma, o modelo aprende a representação vetorial do documento sem levar em consideração as palavras específicas nele contidas. Após o treinamento, cada documento é representado por um vetor de alta dimensionalidade no espaço vetorial. Esses vetores capturam informações contextuais e semânticas do documento, permitindo comparações e análises. O Doc2Vec é amplamente utilizado em várias aplicações de NLP, como classificação de documentos, agrupamento de documentos semelhantes, recuperação de informações, análise de sentimentos e muito mais. Ele permite que os modelos entendam e comparem documentos inteiros, em vez de tratar cada documento como uma coleção independente de palavras.\n",
        "\n",
        "- Essa técnica é especialmente útil em cenários onde o contexto global dos documentos é importante, como na análise de textos longos, artigos científicos, resenhas de produtos, entre outros.\n",
        "\n",
        "**Em resumo, o Doc2Vec é um modelo de NLP que aprende representações vetoriais de documentos inteiros, permitindo uma compreensão mais abrangente e contextual dos textos. Ele é uma extensão poderosa do Word2Vec e tem sido amplamente utilizado para uma variedade de tarefas e aplicações relacionadas ao processamento de linguagem natural**"
      ],
      "metadata": {
        "id": "2O9lWIYbPgbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando doc2vec\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Carregar os dados\n",
        "train_variants = pd.read_csv(\"/content/training_variants\")\n",
        "training_text = pd.read_csv('training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])"
      ],
      "metadata": {
        "id": "o_U1G42FP3QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pré-processamento dos dados\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens = simple_preprocess(text)\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return tokens\n",
        "\n",
        "# Criar os documentos\n",
        "documents = []\n",
        "for idx, row in training_text.iterrows():\n",
        "    doc_id = row['ID']\n",
        "    text = row['Text']\n",
        "    tokens = preprocess_text(text)\n",
        "    documents.append(TaggedDocument(tokens, [doc_id]))"
      ],
      "metadata": {
        "id": "OhV42gnvOHFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Treinar o modelo Doc2Vec\n",
        "vector_size = 100\n",
        "window_size = 5\n",
        "epochs = 20\n",
        "\n",
        "# Modelo\n",
        "model = Doc2Vec(vector_size=vector_size,\n",
        "                window=window_size,\n",
        "                epochs=epochs)\n",
        "\n",
        "# Vocabulario documentos\n",
        "model.build_vocab(documents)\n",
        "\n",
        "# Treinamentos modelo\n",
        "model.train(documents,\n",
        "            total_examples=model.corpus_count,\n",
        "            epochs=model.epochs)"
      ],
      "metadata": {
        "id": "K0gppLcvOHMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de uso do modelo\n",
        "example_text = \"This is an example text for prediction\""
      ],
      "metadata": {
        "id": "8Txmb6sdQaRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando modelo\n",
        "example_tokens = preprocess_text(example_text)\n",
        "example_tokens"
      ],
      "metadata": {
        "id": "GjcZgqWZQmCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando os vectores\n",
        "vector = model.infer_vector(example_tokens)\n",
        "vector"
      ],
      "metadata": {
        "id": "nsSISfKJQaUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similaridade\n",
        "similar_documents = model.docvecs.most_similar([vector])\n",
        "similar_documents"
      ],
      "metadata": {
        "id": "zIKfj65NQqrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extração de tópicos - LDA**\n",
        "\n",
        "\n",
        "Extração de tópicos com LDA (Latent Dirichlet Allocation) é uma técnica utilizada no processamento de linguagem natural (NLP) para identificar tópicos ocultos em um conjunto de documentos. O LDA é um modelo probabilístico que atribui palavras a tópicos e tópicos a documentos, permitindo uma análise estruturada e organizada dos textos. O LDA parte do pressuposto de que cada documento é uma combinação de vários tópicos e que cada tópico é uma distribuição de palavras. Ele assume que a geração dos documentos segue um processo probabilístico latente, em que os tópicos são selecionados aleatoriamente e, em seguida, palavras são escolhidas com base nesses tópicos. O processo de extração de tópicos com LDA envolve as seguintes etapas:\n",
        "\n",
        "A) Pré-processamento: Os documentos são pré-processados para remover stopwords (palavras comuns sem significado específico) e realizar a tokenização, que consiste em separar o texto em unidades menores, como palavras.\n",
        "\n",
        "B) Representação vetorial: Os documentos são transformados em uma representação vetorial, geralmente utilizando a frequência das palavras (bag-of-words) ou técnicas mais avançadas, como o TF-IDF (Term Frequency-Inverse Document Frequency).\n",
        "\n",
        "C) Treinamento do modelo: O modelo LDA é treinado no conjunto de documentos representados vetorialmente. Durante o treinamento, o LDA estima a distribuição dos tópicos em cada documento e a distribuição das palavras em cada tópico. Esse processo é realizado de forma iterativa, ajustando os parâmetros do modelo para maximizar a probabilidade de gerar os documentos observados.\n",
        "\n",
        "D) Inferência de tópicos: Após o treinamento, o modelo LDA pode ser utilizado para inferir os tópicos em novos documentos. Ele atribui probabilidades para cada palavra pertencer a cada tópico, permitindo identificar os tópicos mais relevantes em um documento específico.\n",
        "\n",
        "A extração de tópicos com LDA é amplamente utilizada em várias aplicações de NLP, como agrupamento de documentos, sumarização automática, análise de sentimentos, recomendação de conteúdo e muito mais. Essa técnica ajuda a organizar grandes volumes de texto, identificando temas subjacentes e facilitando a compreensão e o processamento dos documentos. Ao extrair tópicos com LDA, é possível obter uma visão mais estruturada e organizada dos dados textuais, permitindo insights e análises mais precisas em várias áreas de estudo e aplicação."
      ],
      "metadata": {
        "id": "fyq8kej7yZam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar os dados\n",
        "data = pd.read_csv('training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, names=[\"ID\",\"Text\"])\n",
        "data.head()"
      ],
      "metadata": {
        "id": "70J-3X95yd6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pré-processamento dos dados para LDA\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Converter para minúsculas\n",
        "    text = re.sub(r'\\d+', '', text)  # Remover números\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remover pontuação e caracteres especiais\n",
        "    tokens = word_tokenize(text)  # Tokenização\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lematização\n",
        "    tokens = [token for token in tokens if token not in stop_words]  # Remover stopwords\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "# Visualizando o texto novo\n",
        "data['texto_preprocessado'] = data['Text'].apply(preprocess_text)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "aQNJO4iyyQYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vetorização dos dados\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "X = vectorizer.fit_transform(data['texto_preprocessado'])\n",
        "feature_names = vectorizer.get_feature_names_out()  # Correção aqui"
      ],
      "metadata": {
        "id": "42-ADl0byQbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicação do LDA\n",
        "\n",
        "# Defina o número de tópicos desejado\n",
        "num_topics = 10\n",
        "\n",
        "# Modelo LDA\n",
        "lda = LatentDirichletAllocation(n_components=num_topics,\n",
        "                                random_state=42)\n",
        "\n",
        "# Treinamento modelo lda\n",
        "lda.fit(X)"
      ],
      "metadata": {
        "id": "WU0XZmtnzPlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtendo as palavras mais relevantes para cada tópico\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    print(f\"Top 10 palavras para o tópico #{topic_idx}:\")\n",
        "    top_words_indices = topic.argsort()[:-11:-1]\n",
        "    top_words = [feature_names[i] for i in top_words_indices]\n",
        "    print(top_words)"
      ],
      "metadata": {
        "id": "Z4EGEu1CzPoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 5: Atribuição de tópicos aos documentos\n",
        "topic_assignments = lda.transform(X)\n",
        "data['topicos'] = topic_assignments.argmax(axis=1)"
      ],
      "metadata": {
        "id": "5oxw2cBEzcjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de análise dos resultados\n",
        "print(data[['Text', 'topicos']].head(8))  # Exibe os 10 primeiros documentos e seus tópicos atribuídos"
      ],
      "metadata": {
        "id": "t_FoyI4Czcot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar um novo DataFrame com os resultados dos tópicos\n",
        "resultados = pd.DataFrame({'Text': data['Text'], 'topicos': data['topicos']})\n",
        "resultados.head()"
      ],
      "metadata": {
        "id": "Ecojb3lg3B-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Passo 5: Obter as palavras mais relevantes para cada tópico\n",
        "topic_words = []\n",
        "for topic_idx, topic in enumerate(lda.components_):\n",
        "    top_words_indices = topic.argsort()[:-11:-1]\n",
        "    top_words = [feature_names[i] for i in top_words_indices]\n",
        "    topic_words.append(top_words)"
      ],
      "metadata": {
        "id": "08T2XQ3e2Gf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar os tópicos com WordCloud\n",
        "for i, topic_words in enumerate(topic_words):\n",
        "    wordcloud = WordCloud(width=800,\n",
        "                          height=400,\n",
        "                          background_color='white').generate(' '.join(topic_words))\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.title(f'Tópico #{i}')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "S5WsTYp12GjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvar o DataFrame em um arquivo CSV\n",
        "resultados.to_csv('resultados_topicos.csv', index=False)"
      ],
      "metadata": {
        "id": "ClmaVbJn2GnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pré-processamento - ANN**\n",
        "\n",
        "Pré-processamento é uma etapa crucial no processamento de dados, incluindo o processamento de linguagem natural (NLP). É um conjunto de técnicas e procedimentos aplicados aos dados brutos com o objetivo de prepará-los para uma análise mais eficiente e precisa. No contexto do processamento de linguagem natural, o pré-processamento refere-se ao conjunto de transformações aplicadas ao texto antes de ser utilizado em algoritmos ou modelos de aprendizado de máquina. Essas transformações visam limpar, normalizar e estruturar o texto, removendo informações desnecessárias, padronizando formatos e preparando os dados para uma análise mais significativa. Algumas técnicas comuns de pré-processamento incluem:\n",
        "\n",
        "A) Tokenização: Consiste em dividir o texto em unidades menores, chamadas tokens. Esses tokens geralmente são palavras individuais, mas também podem ser frases ou outros elementos, dependendo do objetivo da análise.\n",
        "\n",
        "B) Remoção de stopwords: Stopwords são palavras comuns, como \"o\", \"é\", \"de\", que ocorrem com frequência, mas geralmente não contribuem significativamente para o significado do texto. A remoção dessas palavras pode reduzir o ruído e o tamanho do vocabulário.\n",
        "\n",
        "C) Remoção de pontuação e caracteres especiais: Pontuação, símbolos e outros caracteres especiais podem ser removidos, a menos que tenham um significado específico no contexto da análise.\n",
        "\n",
        "D) Normalização de palavras: Isso envolve a aplicação de técnicas como a lematização (redução de palavras à sua forma base ou lema) e a stemming (redução de palavras à sua raiz) para reduzir a variação morfológica das palavras e agrupar variações semelhantes.\n",
        "\n",
        "E) Remoção de números e dígitos: Se os números não são relevantes para a análise em questão, eles podem ser removidos para simplificar o texto.\n",
        "\n",
        "F) Normalização de caixa: Pode-se converter todas as letras para maiúsculas ou minúsculas, a fim de evitar duplicações desnecessárias de palavras devido à diferenciação de caixa.\n",
        "\n",
        "Essas são apenas algumas técnicas de pré-processamento comuns, e a escolha de quais aplicar depende do objetivo da análise e do tipo de texto sendo processado. O pré-processamento adequado é essencial para melhorar a qualidade dos dados, remover ruídos e garantir que o texto esteja em uma forma mais adequada para análise posterior, como classificação de texto, agrupamento ou extração de informações."
      ],
      "metadata": {
        "id": "3kDFpMohyI_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model_word2vec = None\n",
        "if (model_word2vec == None):\n",
        "    print(\"word2vec não carregado!\")\n",
        "else:\n",
        "    print(\"Encontrados {} vetores de palavras de word2vec\".format(len(model_word2vec.vocab)))"
      ],
      "metadata": {
        "id": "dYFeJQ2nOCMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# definido para o valor máximo do conjunto de dados de origem\n",
        "size = len(train)\n",
        "\n",
        "# verifique a distribuição de classes e encontre o tamanho mínimo da amostra\n",
        "for x in range(1,20):\n",
        "  _= len(train[train[\"Class\"] == x])\n",
        "  if (_ <size):\n",
        "    size =_\n",
        "\n",
        "# Novo dataframe\n",
        "train_x = pd.DataFrame(columns = train.columns)\n",
        "\n",
        "for y in range(1, 20):\n",
        "\n",
        "  # Adicionando nova lista\n",
        "  train_x = train_x.append(train[train['Class']==y][:size], ignore_index=True)\n",
        "\n",
        "def data_tra(l, n):\n",
        "  for i in range(0, len(l), n):\n",
        "    yield l[i:i+n]\n",
        "\n",
        "sets = 200\n",
        "setd = 3\n",
        "\n",
        "base = {'Text': [], 'Class': [], 'ID': [], 'Gene': [], 'Variation': []}\n",
        "for id, row in train.iterrows():\n",
        "  toke_list = nltk.sent_tokenize(row[\"Text\"])\n",
        "  if (len(toke_list) > setd):\n",
        "    toke_list = toke_list[len(toke_list)-sets:]\n",
        "  set_data = list(data_tra(toke_list, setd))\n",
        "\n",
        "  for data2 in set_data:\n",
        "    base['Text'].append(\" \".join(data2))\n",
        "    base['Class'].append(row['Class'])\n",
        "    base['ID'].append(row['ID'])\n",
        "    base['Gene'].append(row['Gene'])\n",
        "    base['Variation'].append(row['Variation'])\n",
        "\n",
        "# Novo dados treinamento\n",
        "datalen = len(train)\n",
        "train = pd.DataFrame(base)\n",
        "train"
      ],
      "metadata": {
        "id": "SgjwSuidc28P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenização das palavras**\n",
        "\n",
        "Tokenização é o processo de dividir um texto em unidades menores chamadas tokens. Esses tokens podem ser palavras individuais, frases, símbolos ou qualquer outra unidade significativa, dependendo do contexto e dos requisitos da análise. A tokenização é uma etapa fundamental no processamento de linguagem natural (NLP) e serve como base para várias tarefas, como classificação de texto, análise de sentimento, tradução automática, entre outras. Ao dividir o texto em tokens, torna-se possível analisar e extrair informações de forma mais granular e eficiente. Aqui estão alguns pontos importantes sobre a tokenização:\n",
        "\n",
        "A) Unidades de token: Os tokens podem ser definidos de diferentes maneiras, dependendo do contexto. A unidade mais comum é a palavra individual. No entanto, em algumas situações, pode ser necessário considerar frases, parágrafos ou até mesmo caracteres individuais como tokens.\n",
        "\n",
        "B) Processo de divisão: A tokenização geralmente envolve a divisão do texto com base em delimitadores, como espaços em branco, pontuação ou outros caracteres específicos. Por exemplo, a frase \"Eu gosto de programação\" pode ser tokenizada em quatro palavras: \"Eu\", \"gosto\", \"de\" e \"programação\".\n",
        "\n",
        "C) Normalização: Em muitos casos, os tokens também são normalizados durante o processo de tokenização. Isso pode incluir a conversão de todas as letras para minúsculas, remoção de acentos ou outras formas de normalização linguística.\n",
        "\n",
        "D) Tratamento de casos especiais: Alguns casos especiais podem surgir durante a tokenização, como contrações (\"não\" + \"é\" = \"não é\"), símbolos ou acrônimos. O tratamento adequado desses casos depende das necessidades da análise e pode exigir regras específicas ou modelos de aprendizado de máquina.\n",
        "\n",
        "A tokenização é uma etapa essencial no pré-processamento de texto em NLP, pois fornece a base para a análise subsequente. Ao dividir o texto em unidades menores, os dados são estruturados de forma mais adequada para análises mais avançadas, permitindo que os algoritmos compreendam e processem o texto de maneira mais eficiente."
      ],
      "metadata": {
        "id": "uEXcQHgShEOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Máximo de palavras\n",
        "n_words = 5000\n",
        "\n",
        "# Comprimento máximo da sequência\n",
        "s_length = 200\n",
        "\n",
        "# Tokenizer\n",
        "token = Tokenizer(num_words = n_words)\n",
        "\n",
        "# Treinamento\n",
        "token_fit = token.fit_on_texts(train['Text'])\n",
        "\n",
        "# Obter sequências\n",
        "X = token.texts_to_sequences(train['Text'])\n",
        "\n",
        "# Palavras únicas no texto\n",
        "word_index = token.word_index\n",
        "print(\"Found {} unique tokens.\".format(len(word_index)))\n",
        "\n",
        "# Sequências de pad\n",
        "X = pad_sequences(X, maxlen=s_length)\n",
        "\n",
        "# Embedding text\n",
        "emb = None\n",
        "\n",
        "# Modelo word2vec\n",
        "if (model_word2vec != None):\n",
        "  words = []\n",
        "  emb = np.zeros((num_words+1, 200)) #200 = word2vec dim\n",
        "  for x in word_index.items():\n",
        "    if x > n_words:\n",
        "      continue\n",
        "    if x in model_word2vec.vocab:\n",
        "      emb[i] = model_word2vec(x)\n",
        "    else:\n",
        "      words.append(x)"
      ],
      "metadata": {
        "id": "AJgST1KAdXya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rede neural**\n",
        "\n",
        "\n",
        "Uma rede neural para processamento de linguagem natural (NLP) é um tipo de modelo de aprendizado de máquina que é projetado para lidar especificamente com tarefas relacionadas ao processamento e compreensão de linguagem humana. Essas redes neurais são treinadas em grandes conjuntos de dados textuais para aprender padrões, relações semânticas e sintáticas, e realizar tarefas específicas relacionadas à linguagem. Existem várias arquiteturas de redes neurais usadas no processamento de linguagem natural, cada uma com seus próprios pontos fortes e aplicações. Aqui estão algumas das arquiteturas mais comuns:\n",
        "\n",
        "A) Redes Neurais Recorrentes (RNN): As RNNs são adequadas para lidar com sequências de dados, como frases ou documentos. Elas possuem conexões recursivas que permitem que a informação seja passada de um passo de tempo para outro, capturando assim o contexto e a dependência entre as palavras.\n",
        "\n",
        "B) Long Short-Term Memory (LSTM): As LSTMs são uma variação das RNNs, projetadas para superar o problema de dependências de longo prazo. Elas têm uma estrutura de memória interna que permite reter informações relevantes por períodos mais longos, sendo úteis para tarefas de linguagem com dependências de longa distância, como tradução automática ou geração de texto.\n",
        "\n",
        "C) Redes Neurais Convolucionais (CNN): As CNNs, originalmente desenvolvidas para visão computacional, também são aplicadas com sucesso em NLP. Elas podem ser usadas para extrair recursos locais, como n-gramas, de sequências de palavras, sendo úteis para tarefas como classificação de texto e análise de sentimentos.\n",
        "\n",
        "D) Transformers: Os Transformers são uma arquitetura mais recente e altamente popular no processamento de linguagem natural. Eles se destacam por sua capacidade de capturar relações de dependência entre palavras, eliminando a necessidade de conexões recorrentes. Os Transformers são amplamente utilizados para tarefas como tradução automática, resumo de texto e geração de texto.\n",
        "\n",
        "Essas são apenas algumas das arquiteturas de redes neurais usadas no processamento de linguagem natural. Cada uma delas possui características distintas e é aplicada de acordo com a tarefa específica e os requisitos do problema em questão. As redes neurais para processamento de linguagem natural têm sido fundamentais para avanços significativos em áreas como tradução automática, análise de sentimentos, processamento de fala, chatbots e muito mais."
      ],
      "metadata": {
        "id": "uwYbkJZFg1OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rede neural\n",
        "\n",
        "# Word2vec entrada\n",
        "dim = 200\n",
        "\n",
        "# Modelo\n",
        "model_file_name = 'model'\n",
        "\n",
        "# Dados treino\n",
        "Y = train['Class'].values-1\n",
        "\n",
        "# One hot com dados coluna target\n",
        "Y = keras.utils.to_categorical(Y)\n",
        "\n",
        "# Aumento do tamanho do lote no ambiente local\n",
        "batch_size = 20\n",
        "\n",
        "# Épocas da rede\n",
        "epochs = 100\n",
        "\n",
        "# salvamento de modelo com callback\n",
        "ckpt_callback = keras.callbacks.ModelCheckpoint(model_file_name, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "\n",
        "# input layer\n",
        "input1 = keras.layers.Input(shape=(s_length,))\n",
        "\n",
        "# camada de incorporação\n",
        "if (emb == None):\n",
        "    # word2vec não foi carregado. usar método alternativo\n",
        "    embedding = keras.layers.Embedding(n_words+1, dim, trainable=True)(input1)\n",
        "else:\n",
        "    # word2vec foi carregado, pesos de carga e definido como não treinável\n",
        "    embedding = keras.layers.Embedding(n_words+1, dim, weights=[emb], trainable=False)(input1)\n",
        "\n",
        "# camadas de conversão ANN\n",
        "convs = []\n",
        "filter_sizes = [2,3,4]\n",
        "for fsz in filter_sizes:\n",
        "    l_conv = keras.layers.Conv1D(filters=100,kernel_size=fsz,activation='relu')(embedding)\n",
        "    l_pool = keras.layers.MaxPooling1D(s_length-100+1)(l_conv)\n",
        "    l_pool = keras.layers.Flatten()(l_pool)\n",
        "    convs.append(l_pool)\n",
        "\n",
        "# ANN\n",
        "l_merge = keras.layers.concatenate(convs, axis=1)\n",
        "l_out = keras.layers.Dropout(0.5)(l_merge)\n",
        "output = keras.layers.Dense(units=9, activation='softmax')(l_out)\n",
        "model = keras.models.Model(input1, output)\n",
        "\n",
        "# compile model\n",
        "ANN_model = model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['categorical_crossentropy'])\n",
        "\n",
        "# Summary ANN\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "i7DEEea1fIAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento modelo\n",
        "ANN_model = model.fit(X,\n",
        "                      Y,\n",
        "                      epochs=epochs,\n",
        "                      batch_size=batch_size,\n",
        "                      validation_split=0.2,\n",
        "                      verbose=1)"
      ],
      "metadata": {
        "id": "Gz0skQbNhdRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training accuracy: %.2f%% / Validation accuracy: %.2f%%\" % (100*ANN_model.history['categorical_crossentropy'][-1], 100*ANN_model.history['val_categorical_crossentropy'][-1]))"
      ],
      "metadata": {
        "id": "Ilx5t_xGjWNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history\n",
        "plt.plot(ANN_model.history['val_loss'])\n",
        "plt.plot(ANN_model.history['val_categorical_crossentropy'])\n",
        "plt.title('Precisão do modelo')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history loss\n",
        "plt.plot(ANN_model.history['loss'])\n",
        "plt.plot(ANN_model.history['val_loss'])\n",
        "plt.title('Perda do modelo')\n",
        "plt.ylabel('Perda')\n",
        "plt.xlabel('Época')\n",
        "plt.legend(['train', 'valid'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NSCFsxkqjWR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dados treino ANN**"
      ],
      "metadata": {
        "id": "r8IfBRv7jwdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dados teste\n",
        "data_test = pd.read_csv('/content/training_variants')\n",
        "\n",
        "# Carregando dados texto\n",
        "test_txt_ = pd.read_csv('stage2_test_text.csv',\n",
        "                        sep='\\|\\|',\n",
        "                        engine='python',\n",
        "                        header=None,\n",
        "                        skiprows=1,\n",
        "                        names=[\"ID\",\"Text\"])\n",
        "\n",
        "# Carregando csv dados teste\n",
        "test_data = pd.merge(data_test,\n",
        "                     test_txt_,\n",
        "                     how='left',\n",
        "                     on='ID')"
      ],
      "metadata": {
        "id": "N0o2qQiMjv6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tmpdf_ = {'Text': [], 'ID': [], 'Gene': [], 'Variation': []}\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    toke = nltk.sent_tokenize(row['Text'])\n",
        "    if (len(toke) > sets):\n",
        "        toke = toke[len(toke)-sets:]\n",
        "    data_r = list(data_tra(toke, setd))\n",
        "    for chunk in data_r:\n",
        "      tmpdf_['Text'].append(\" \".join(chunk))\n",
        "      tmpdf_['ID'].append(row['ID'])\n",
        "      tmpdf_['Gene'].append(row['Gene'])\n",
        "      tmpdf_['Variation'].append(row['Variation'])\n",
        "\n",
        "# create new train set from temp dict\n",
        "origtestlen = len(test)\n",
        "test = pd.DataFrame(tmpdf_)\n",
        "\n",
        "# clean up\n",
        "del tmpdf_\n",
        "\n",
        "# Visualizando\n",
        "test"
      ],
      "metadata": {
        "id": "zdDWXrM4mjah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get sequences\n",
        "Xtest = token.texts_to_sequences(test['Text'])"
      ],
      "metadata": {
        "id": "9UKYKabGjWXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pad sequences\n",
        "Xtest = pad_sequences(Xtest, maxlen=s_length)"
      ],
      "metadata": {
        "id": "n75g7YUQkSol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsão rede neural\n",
        "pred = model.predict(Xtest, verbose=1)\n",
        "print(pred)"
      ],
      "metadata": {
        "id": "jevtjWq1kdiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resultado final ANN salvando csv\n",
        "data_final = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n",
        "\n",
        "# Incluindo ids\n",
        "data_final.insert(loc=0, column='ID', value=test['ID'].values)\n",
        "\n",
        "# Media resultados\n",
        "data_final = data_final.groupby(['ID'], as_index=False).mean()"
      ],
      "metadata": {
        "id": "NaysSMLdklHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizando os resultados\n",
        "data_final"
      ],
      "metadata": {
        "id": "azW8KikxkwlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Salvando em csv dataset\n",
        "data_final.to_csv('dataset_resultados_final.csv', index=False)"
      ],
      "metadata": {
        "id": "f1Lc-xTglK5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusão**\n",
        "\n",
        "\n",
        "Neste projeto, desenvolvi uma abordagem utilizando redes neurais para classificar variações genéticas. Para enriquecer a representação dos dados, utilizei modelos de processamento de linguagem natural, como Word2Vec e Doc2Vec, para extrair informações relevantes das palavras-chave presentes nos textos. Esses modelos permitiram capturar relações semânticas e contextuais, contribuindo para uma melhor compreensão das variações genéticas.\n",
        "\n",
        "Além disso, como uma etapa adicional, implementei um modelo de extração de tópicos usando o LDA (Latent Dirichlet Allocation). Essa técnica identifica tópicos latentes presentes nos dados e auxilia na compreensão global do corpus. Ao final deste processo, utilizei um gráfico de nuvem de palavras para visualizar as palavras-chave mais relevantes em cada tópico, oferecendo uma representação visual clara e concisa.\n",
        "\n",
        "Por fim, implementei um terceiro modelo baseado em rede neural, que foi treinado para classificar as variações genéticas. Esse modelo foi projetado para aprender padrões complexos e realizar uma classificação precisa. A rede neural recebeu como entrada as informações extraídas dos modelos anteriores (Word2Vec, Doc2Vec e LDA), o que proporcionou um conjunto de características mais abrangente para a classificação.\n",
        "\n",
        "O processo de desenvolvimento envolveu a coleta e preparação dos dados genéticos, incluindo a limpeza e padronização dos textos. Em seguida, os modelos de processamento de linguagem natural foram aplicados para extrair informações das palavras-chave e identificar os tópicos relevantes. Essas informações foram utilizadas como entrada para o modelo de rede neural, que foi treinado utilizando técnicas avançadas de aprendizado profundo.\n",
        "\n",
        "Durante o treinamento do modelo de rede neural, foram aplicadas técnicas de validação cruzada e otimização para garantir que o modelo estivesse bem ajustado aos dados e fosse capaz de realizar uma classificação precisa das variações genéticas.\n",
        "\n",
        "Em resumo, este projeto empregou uma abordagem multifacetada, utilizando modelos de processamento de linguagem natural (Word2Vec, Doc2Vec e LDA) e redes neurais para a classificação de variações genéticas. Essa combinação de técnicas permitiu uma representação mais rica e abrangente dos dados, contribuindo para resultados mais precisos e confiáveis.\n"
      ],
      "metadata": {
        "id": "qRp11SBFmc19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Referência**\n",
        "\n",
        "Link 1 - https://medium.com/@everton.tomalok/word2vec-e-sua-import%C3%A2ncia-na-etapa-de-pr%C3%A9-processamento-d0813acfc8ab\n",
        "\n",
        "Link 2 - https://medium.com/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e\n",
        "\n",
        "Link 3 - https://www.kaggle.com/code/umutto/preliminary-data-analysis-using-word2vec/notebook\n",
        "\n",
        "Link 4 - https://www.kaggle.com/code/bostjanm/cnn-aproach-to-text-mining\n",
        "\n",
        "Link 5 - https://www.kaggle.com/competitions/msk-redefining-cancer-treatment"
      ],
      "metadata": {
        "id": "tPoaFl28mdKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Citação**\n",
        "\n",
        "Esse projeto da formação cientista de dados pela DSA.\n",
        "\n",
        "@Data Science Academy\n",
        "\n",
        "https://www.datascienceacademy.com.br/"
      ],
      "metadata": {
        "id": "ccQFYYXyxaxe"
      }
    }
  ]
}